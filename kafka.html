<!DOCTYPE html>
<html lang="en">
<head>
    <title>Kafka</title>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- Stylesheet -->
    <link rel="stylesheet" href="js/styles.css"/>
</head>
<body data-sidebar="kafka">
<nav>
    <button class="mobile-sidebar-btn" id="mobileSidebarBtn">☰</button>
    <div class="breadcrumbs">
        <a href="./home.html">Home</a>
        <span class="separator">/</span>
        <a href="#">Kafka</a>
    </div>
</nav>
<!-- Components will be inserted here by components.js -->

<!-- Main Content Section -->
<main id="content">
    <h2>1. Introduction to Apache Kafka</h2>

    <p>Apache Kafka is an <strong>open-source, distributed event streaming platform</strong> designed for
        high-performance data pipelines, streaming analytics, data integrations, and mission-critical applications. It's
        optimized for ingesting and processing streaming data in real-time.</p>

    <p><strong>Event streaming</strong> is defined as "the practice of capturing the data in real time from event
        sources like databases, sensors, mobile devices, cloud services, and software applications in in the form of
        streams of events."</p>
    <hr>
    <h2>2. Key Features and Advantages of Kafka</h2>

    <p>Kafka boasts several powerful features that make it a robust solution for real-time data processing:</p>

    <ol>
        <li><strong>High Scalability</strong>: Kafka is designed for <strong>horizontal scalability</strong>, allowing
            it to handle massive amounts of data by adding more hardware or nodes to a cluster without downtime.
        </li>

        <li><strong>High Throughput & Low Latency</strong>: <strong>Throughput</strong> is "a measure of how many units
            of information a system can process in a given amount of time." Kafka can handle "millions of messages per
            second."
        </li>

        <li><strong>Latency</strong> is "the delay in the network communication." Kafka achieves low latency (as low as
            2 milliseconds max) by storing data in memory, enabling quick read and write operations.
        </li>

        <li><strong>Distributed Systems</strong>: Kafka's distributed architecture ensures <strong>fault tolerance and
            reliability</strong>. Data is distributed across multiple nodes, reducing the risk of data loss.
        </li>

        <li><strong>Publish-Subscribe Messaging System</strong>: Kafka operates as a pub-sub system where "producer can
            publish the messages or data to the topics and consumer subscribes to those topics to receive the data."
        </li>

        <li><strong>Retention and Durability</strong>: Kafka offers <strong>configurable retention periods</strong> for
            messages, ensuring data durability over time. Data can be stored for a specified duration or size, allowing
            for data replay or reprocessing.
        </li>

        <li><strong>Support for Stream Processing</strong>: Kafka provides a streaming platform for building real-time
            data pipelines and applications, with the Kafka Streams API enabling processing within its ecosystem.
        </li>

        <li><strong>Connectivity and Integrations</strong>: Kafka supports connectors for seamless integration with
            various data sources and sinks, facilitating data transfer between systems.
        </li>
    </ol>
    <hr>
    <h2>3. Kafka Use Cases</h2>

    <p>Kafka is widely used across various industries for:</p>

    <ol>
        <li><strong>Financial Transactions</strong>: Processing payments and financial transactions in real-time (e.g.,
            stock exchanges, banks, insurance).
        </li>

        <li><strong>Logistics & Automotive</strong>: Tracking and monitoring parts, shipments, and vehicles in
            real-time.
        </li>

        <li><strong>IoT Data</strong>: Continuously capturing and analyzing sensor data from IoT devices and other
            equipment (e.g., factories, wind parks).
        </li>

        <li><strong>E-commerce & Retail</strong>: Collecting and reacting to customer interactions in real-time (e.g.,
            e-commerce, order processing, hotel/travel).
        </li>

        <li><strong>Healthcare</strong>: Monitoring patients and predicting condition changes for timely treatment in
            emergencies.
        </li>

        <li><strong>Data Integration</strong>: Connecting, storing, and making data available across different company
            divisions.
        </li>

        <li><strong>Architectural Foundations</strong>: Serving as the foundation for data platforms, event-driven
            architectures, and microservices.
        </li>
    </ol>
    <hr>
    <h2>4. Kafka Key Components</h2>

    <p>Understanding these core concepts is crucial for working with Kafka:</p>

    <ol>
        <li><strong>Kafka Clusters</strong>: A set of Kafka brokers working together. One broker acts as a controller to
            manage cluster metadata.
        </li>

        <li><strong>Brokers</strong>: A Kafka server that stores data and serves clients. A Kafka cluster consists of
            multiple brokers for load balancing. Brokers are stateless and use replication for fault tolerance and data
            durability.
        </li>

        <li><strong>Topics</strong>: A logical channel to which messages are sent and from which messages are consumed.
            It can have multiple partitions.
        </li>

        <li><strong>Partitions</strong>: "Each topic is divided into one or more partitions, the basic unit of
            parallelism and distribution in the cough." Messages within a partition are ordered and assigned an offset
            for tracking.
        </li>

        <li><strong>Partitions Offset</strong>: "A unique ID for each message in a partition. Consumers use offsets to
            track read progress." Offsets are crucial for consumers to track their position, know which messages they've
            consumed, and resume consumption from the last committed offset if they crash or restart.
            <p>An offset is a unique identifier for a message within a partition.</p>
            <p>Kafka stores messages in partitions in a sequential log.</p>
            <p>Each message in a partition has an increasing offset like 0, 1, 2, 3, ....</p>
            <p>Consumers track offsets to know what message to read next.</p>
    </ol>

    <pre class="java-code">
<code>
props.put(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, "earliest");
</code>
</pre>

    <div class="table-container">
        <table class="basic-table">
            <thead>
            <tr>
                <th>Offset</th>
                <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td>Earliest</td>
                <td>Starts consuming from the beginning of the partition (offset 0).</td>
            </tr>
            <tr>
                <td>Latest (default)</td>
                <td>Starts consuming only new messages (skip old).</td>
            </tr>
            <tr>
                <td>Manual</td>
                <td>You can explicitly seek to a specific offset in code.</td>
            </tr>
            </tbody>
        </table>
    </div>

    <pre class="code-block">
<code>
<span class="key">spring:</span>
  <span class="key">kafka:</span>
    <span class="key">consumer:</span>
      <span class="key">group-id:</span> <span class="value">payment-group</span>
      <span class="key">auto-offset-reset:</span> <span class="value">earliest</span> <span
        class="comment"># or latest</span>
</code>
</pre>
    </li>
    <ol start="6">
        <li><strong>Producers</strong>: A client that publishes (writes) data (messages/events) to one or more Kafka
            topics. Producers decide which partition to write to. Producers send messages asynchronously and serialize
            data into a byte format.
        </li>

        <li><strong>Consumers</strong>: A client that subscribes (reads) messages from one or more Kafka topics. Can be
            part of a consumer group for load balancing.
            <p>Consumers always read data from lower to higher offsets.</p>
            <p>If a consumer reads from multiple partitions, message order is not guaranteed across partitions.</p>
            <p>Consumers deserialize data from byte format into usable data structures.</p>
            <p>Consumers are assigned to specific partitions for parallel processing and scalability.</p>
            <p>Consumers acknowledge message receipt and commit offsets to ensure reliable, at-least-once message
                delivery.</p>
        </li>
        <li><strong>Acknowledgement Settings (acks)</strong> Producers can configure acks to control message delivery
            durability and reliability:

            <p><strong>acks=0:</strong> Producer sends data without waiting for acknowledgement. Possible data loss.</p>

            <p><strong>acks=1</strong> (Default in Kafka 2.0+): Leader broker acknowledges receipt. Limited data loss
                possible
                if leader fails before replicas synchronize.</p>

            <p><strong>acks=all:</strong> Leader waits for all in-sync replicas to acknowledge before responding to the
                producer. No data loss.</p>
        </li>
        <li><strong>Message Keys</strong>
            partitions in a
            round-robin fashion. If a key is provided, messages with the same key are always sent to and stored
            in the
            same
            Kafka partition, ensuring order.
        </li>

        <li><strong>Consumer Group</strong>

            "Consumers are organized into consumer groups...where each group read from a topic." A consumer
            group can
            contain
            multiple consumers, processing subsets of messages, which facilitates scalability and load
            distribution.
            <p><strong>Consumer Behavior with Partitions: Single Consumer, Multiple Partitions:</strong> A
                single consumer
                in a
                group will consume all partitions of a topic.</p>

            <p><strong>Multiple Consumers, Multiple Partitions (Consumers <= Partitions):</strong> Kafka
                auto-balances,
                assigning partitions to consumers for load distribution. Each partition is consumed by only one
                consumer
                within
                a group at a time.</p>

            <p><strong>Multiple Consumers, Multiple Partitions (Consumers > Partitions):</strong> All partitions
                will be
                consumed, and any excess consumers will sit idle.</p>

            <p><strong>Multiple Consumer Groups:</strong> Each consumer group independently consumes from the
                topic's
                partitions.</p>

            <img src="js/images/kafka/consumer_grp.png" class="responsive-img">
        </li>
        <li><strong>Leader/Follower Replications</strong>
            Each partition has one leader and multiple followers. Leaders handle all read and write
            requests for their
            partition, and followers replicate the leader's data. If a leader fails, a follower becomes
            the new leader.
            leaders perform all reads and writes to a particular topic partition. followers replicate
            leaders.
        </li>
        <img src="js/images/kafka/replica.png" class="responsive-img">
    </ol>
    <hr>
    <h2>5. Zookeeper and Kafka (Legacy vs. Current)</h2>

    <ul>
        <li><strong>Zookeeper (Legacy):</strong> Traditionally, Zookeeper acted as a distributed coordination service in
            Kafka, managing and coordinating Kafka brokers. It notified producers and consumers about new or failed
            brokers, facilitating tasks like metadata management, leader election, consumer group coordination, and
            configuration management. Kafka would get offset values from Zookeeper.
        </li>

        <li><strong>Newer Versions (Kafka 3.3+):</strong> "Apache Kafka 3.3 replaces Zookeeper with the new care of
            consensus protocol," specifically <strong>Kaft (Kafka Raft metadata mode)</strong>. In this model, "each
            broker will have a Quorum controller," which handles the reading and writing of events to a dedicated
            metadata topic within the Kafka cluster itself, eliminating the Zookeeper dependency.
        </li>
    </ul>

    <img src="js/images/kafka/arc.png" class="responsive-img">
    <hr>
    <h2>6. Spring Boot Kafka Integration</h2>

    <p>The tutorial demonstrates how to integrate Apache Kafka with Spring Boot, focusing on setting up producers and
        consumers.</p>

    <p><strong>Prerequisites: </strong> Kafka needs to be installed and running (version 3.6.1 used, supporting Kaft).
    </p>

    <h3>6.1 Running Kafka with Kraft</h3>

    <p><strong>Generate Cluster UUID: </strong> kafka-storage.sh random-uuid (generates a unique identifier for the
        cluster).</p>

    <p><strong>Format Log Directories: </strong> kafka-storage.sh format -t &lt;UUID&gt; -c &lt;path_to_server.properties&gt;
    </p>

    <p><strong>Start Kafka Server: </strong> kafka-server-start.sh &lt;path_to_server.properties&gt;</p>
    <hr>
    <h3>6.2 Producer Configuration and Service</h3>

    <p><strong>Dependencies: </strong> Spring Web and Kafka dependencies are required.</p>

    <pre class="code-block">
<code>
<span class="tag">&lt;dependency&gt;</span>
    <span class="tag">&lt;groupId&gt;</span>org.springframework.kafka<span class="tag">&lt;/groupId&gt;</span>
    <span class="tag">&lt;artifactId&gt;</span>spring-kafka<span class="tag">&lt;/artifactId&gt;</span>
<span class="tag">&lt;/dependency&gt;</span>
</code>
</pre>

    <p><strong>ProducerConfiguration Class: </strong> Defines three beans:</p>

    <ol>
        <li><strong>ProducerFactory: </strong> An interface used to create Kafka producer instances, enabling consistent
            configuration
            across Spring Boot applications. It's configured with bootstrap.servers, key.serializer, and
            value.serializer.
        </li>

        <li><strong>KafkaTemplate: </strong> Simplifies Kafka producer usage by providing a higher-level abstraction for
            common operations
            like sending messages (synchronously and asynchronously).
        </li>

        <li><strong>NewTopic: </strong> Creates a Kafka topic with specified name, number of partitions, and replication
            factor.
        </li>
    </ol>

    <pre class="java-code">
<code>
<span class="annotation">@Configuration</span>
<span class="keyword">public class</span> <span class="class">ProducerConfiguration</span> {
    <span class="comment">//ProducerFactory</span>
    <span class="annotation">@Bean</span>
    <span class="keyword">public</span> <span class="class">ProducerFactory</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="method">producerFactory</span>(){
    <span class="class">Map</span>&lt;<span class="class">String</span>,<span class="class">Object</span>&gt; <span
        class="var">configMaps</span>= <span class="keyword">new</span> <span class="class">HashMap</span>&lt;&gt;();
    <span class="var">configMaps</span>.put(<span class="class">ProducerConfig</span>.BOOTSTRAP_SERVERS_COMFIG,<span
        class="string">"localhost:9092"</span>);
    <span class="var">configMaps</span>.put(<span class="class">ProducerConfig</span>.KEY_SERIALIZER_CLASS_COMFIG, <span
        class="class">StringSerializer</span>.class);
    <span class="var">configMaps</span>.put(<span
        class="class">ProducerConfig</span>.VALUE_SERIALIZER_CLASS_COMFIG, <span class="class">StringSerializer</span>.class);
    <span class="keyword">return</span> <span class="keyword">new</span> <span
        class="class">DefaultKafkaProducerFactory</span>&lt;&gt;(<span class="var">configMaps</span>);
    }
    <span class="comment">//KafkaTemplate</span>
    <span class="annotation">@Bean</span>
    <span class="keyword">public</span> <span class="class">KafkaTemplate</span>&lt;<span
        class="class">String</span>,<span class="class">String</span>&gt; <span class="method">kafkaTemplate</span>(){
    <span class="keyword">return</span> <span class="keyword">new</span> <span class="class">KafkaTemplate</span>&lt;&gt;(<span
        class="method">producerFactory</span>());
    }
    <span class="comment">//NewTopic</span>
    <span class="annotation">@Bean</span>
    <span class="keyword">public</span> <span class="class">NewTopic</span> <span class="method">paymentTopic</span>(){
    <span class="keyword">return</span> <span class="keyword">new</span> <span class="class">NewTopic</span>(<span
        class="string">"payment-topic"</span>,<span class="number">3</span>,(<span class="keyword">short</span>) <span
        class="number">1</span>);
    }
}
</code>
</pre>

    <div class="table-container">
        <table class="basic-table">
            <thead>
            <tr>
                <th>Port</th>
                <th>Description</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <th>"payment-topic"</th>
                <th>Name of the topic being created. In this case, the topic is called "payment-topic".</th>
            </tr>
            <tr>
                <td>3</td>
                <td>Number of partitions for the topic. This means the topic will be split into 3 partitions. Each
                    partition can handle part of the data for parallelism and scalability.
                </td>
            </tr>
            <tr>
                <td>(short) 1</td>
                <td>Replication factor. The number 1 (cast to short) means each partition will have 1 copy (no
                    replication). Normally in production, this is ≥ 2 for fault tolerance.
                </td>
            </tr>
            </tbody>
        </table>
    </div>

    <p><strong>ProducerService Class:</strong> Autodires KafkaTemplate.</p>

    <p>Includes methods to generate random payment transactions.</p>

    <pre class="java-code">
<code>
<span class="annotation">@Service</span>
<span class="annotation">@Slf4j</span>
<span class="keyword">public class</span> <span class="class">ProducerService</span> {
    <span class="annotation">@Autowired</span>
    <span class="keyword">private</span> <span class="class">KafkaTemplate</span>&lt;<span
        class="class">String</span>,<span class="class">String</span>&gt; <span class="var">KafkaTemplate</span>;
    <span class="comment">//generateRandomTransaction</span>
    <span class="keyword">public</span> <span class="class">String</span> <span
        class="method">generateRandomTransaction</span>(){
        <span class="class">String</span> <span class="var">vendors</span>[]={<span class="string">"Amazon"</span>,<span
        class="string">"Paypal"</span>,<span class="string">"Visa"</span>,<span class="string">"mastercard"</span>};
        <span class="class">String</span> <span class="var">vendor</span> = <span class="var">vendors</span>[<span
        class="class">ThreadLocalRandom</span>.current().nextInt(<span class="var">vendors</span>.length)];
        <span class="keyword">double</span> <span class="var">amount</span> = <span
        class="class">ThreadLocalRandom</span>.current().nextDouble(<span class="number">0.10</span>,<span
        class="number">1000.0</span>);
        <span class="keyword">return</span> <span class="string">"Vendor: "</span>+<span class="var">vendor</span>+<span
        class="string">"Amount $"</span>+<span class="var">amount</span>;
    }
    <span class="comment">//SendPaymentTransactions Asynchronously</span>
    <span class="comment">// @Schedule(fixedRate = 2000)</span>
    <span class="keyword">public</span> <span class="keyword">void</span> <span class="method">SendPaymentTransactionsAsynchronously</span>(){
        <span class="class">String</span> <span class="var">transaction</span> = <span class="method">generateRandomTransaction</span>();
        log.info(<span class="string">"Sending payment transactions {}"</span>,<span class="var">transaction</span>);
        <span class="var">kafkaTemplate</span>.send(<span class="string">"payment-topic"</span>,<span class="var">transaction</span>)
        .whenComplete(((<span class="var">sendResult</span>, <span class="var">throwable</span>) -&gt; {
            <span class="keyword">if</span>(<span class="var">throwable</span>!=<span class="keyword">null</span>){
                <span class="method">onFailure</span>(<span class="var">throwable</span>);
            }<span class="keyword">else</span> {
                <span class="method">onSuccess</span>(<span class="var">sendResult</span>);
            }
        }));
    }
    <span class="keyword">private</span> <span class="keyword">void</span> <span class="method">onSuccess</span>(<span
        class="class">SendResult</span>&lt;<span class="class">String</span>, <span
        class="class">String</span>&gt; <span class="var">sendResult</span>) {
        log.info(<span class="string">"Received new metadata. \n"</span> +
        <span class="string">"Topic: {}, Partition: {}, Offset: {}, Timestamp: {}"</span>,
        <span class="var">sendResult</span>.getRecordMetadata().topic(),
        <span class="var">sendResult</span>.getRecordMetadata().partition(),
        <span class="var">sendResult</span>.getRecordMetadata().offset(),
        <span class="var">sendResult</span>.getRecordMetadata().timestamp());
    }
    <span class="keyword">private</span> <span class="keyword">void</span> <span class="method">onFailure</span>(<span
        class="class">Throwable</span> <span class="var">throwable</span>) {
        log.info(<span class="string">"Error occurred while producing the message {}"</span>, <span class="var">throwable</span>);
    }
    <span class="keyword">public</span> <span class="class">String</span> <span
        class="method">generateTransactionKey</span>(){
        <span class="keyword">return</span> <span class="class">UUID</span>.randomUUID().toString();
    }
    <span class="comment">//SendPaymentTransactions-2)synchronously</span>
    <span class="annotation">@ScheduleoffixedRate</span> = <span class="number">2000</span>)
    <span class="keyword">public</span> <span class="class">SendResult</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="method">SendPaymentTransactionsSynchronously</span>() <span
        class="keyword">throws</span> <span class="class">ExecutionException</span>,
    <span class="class">InterruptedException</span> {
        <span class="class">String</span> <span class="var">transaction</span>= <span class="method">generateRandomTransaction</span>();
        log.info(<span class="string">"Sending payment transactions {}"</span>,<span class="var">transaction</span>);
        <span class="class">SendResult</span>&lt;<span class="class">String</span>,<span class="class">String</span>&gt; <span
        class="var">sendResult</span> = <span class="var">kafkaTemplate</span>.send(<span
        class="string">"payment-topic"</span>,<span class="method">generateTransactionKey</span>(),<span class="var">transaction</span>).get();
        log.info(<span class="string">"Received new metadata. \n"</span> +
        <span class="string">"Topic: {}, Partition: {}, Offset: {}, timestamp: {}"</span>,
        <span class="var">sendResult</span>.getRecordMetadata().topic(),
        <span class="var">sendResult</span>.getRecordMetadata().partition(),
        <span class="var">sendResult</span>.getRecordMetadata().offset(),
        <span class="var">sendResult</span>.getRecordMetadata().timestamp());
        <span class="keyword">return</span> <span class="var">sendResult</span>;
    }
}
</code>
</pre>

    <p><strong>Sending Messages:Asynchronously:</strong> kafkaTemplate.send(topic, key, value) returns a
        ListenableFuture. A whenComplete
        callback is used to process the SendResult (containing metadata like topic, partition, offset, timestamp) on
        success or handle exceptions on failure.</p>

    <p><strong>Sending Messages:Synchronously: </strong> kafkaTemplate.send(topic, key, value).get() blocks until the
        message is sent and returns the
        SendResult.</p>

    <p><strong>Scheduling: </strong> Uses @ScheduleoffixedRate = 2000) to send transactions every 2 seconds.</p>

    <p><strong>Specific Partition Sending: </strong> kafkaTemplate.send(topic, partition, key, value) allows explicitly
        specifying the
        target partition.</p>

    <pre class="java-code">
<code>
<span class="comment">// Case 1: Send to specific partition, here 2</span>
<span class="var">kafkaTemplate</span>.send(<span class="string">"payment-topic"</span>, <span class="number">2</span>, <span
        class="string">"txn123"</span>, <span class="string">"Rs 100 paid"</span>);

<span class="comment">// Case 2: Send with key (Kafka decides partition using key hash)</span>
<span class="var">kafkaTemplate</span>.send(<span class="string">"payment-topic"</span>, <span
        class="string">"txn123"</span>, <span class="string">"Rs 200 paid"</span>);

<span class="comment">// Case 3: Send without key (Kafka round-robins the partition)</span>
<span class="var">kafkaTemplate</span>.send(<span class="string">"payment-topic"</span>, <span class="string">"Rs 300 paid"</span>);
</code>
</pre>
    <hr>
    <h3>6.3 Consumer Configuration and Service</h3>

    <p><strong>Dependencies: </strong> Similar to producer, Spring Web and Kafka are needed.</p>

    <pre class="code-block">
<code>
<span class="tag">&lt;dependency&gt;</span>
<span class="tag">&lt;groupId&gt;</span>org.springframework.kafka<span class="tag">&lt;/groupId&gt;</span>
<span class="tag">&lt;artifactId&gt;</span>spring-kafka<span class="tag">&lt;/artifactId&gt;</span>
<span class="tag">&lt;/dependency&gt;</span>
</code>
</pre>

    <p>ConsumerConfiguration Class:Defines two beans:</p>

    <ol>
        <li><strong>ConsumerFactory: </strong> Configured with bootstrap.servers, key.deserializer, and
            value.deserializer.
        </li>

        <li><strong>ConcurrentKafkaListenerContainerFactory: </strong> Used to create KafkaMessageListenerContainer
            instances that manage
            Kafka consumers. It's configured with the ConsumerFactory.
        </li>
    </ol>

    <pre class="java-code">
<code>
<span class="annotation">@Configuration</span>
<span class="annotation">@EnableKafka</span>
<span class="keyword">public class</span> <span class="class">ConsumerConfiguration</span> {

    <span class="comment">//ConsumerFactory</span>

    <span class="annotation">@Bean</span>
    <span class="keyword">public</span> <span class="class">ConsumerFactory</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="method">consumerFactory</span>() {
        <span class="class">Map</span>&lt;<span class="class">String</span>,<span class="class">Object</span>&gt; <span
        class="var">configMap</span> = <span class="keyword">new</span> <span class="class">HashMap</span>&lt;&gt;();
        <span class="var">configMap</span>.put(<span class="class">ConsumerConfig</span>.BOOTSTRAP_SERVERS_COMFIG,<span
        class="string">"localhost:9092"</span>);
        <span class="var">configMap</span>.put(<span class="class">ConsumerConfig</span>.KEY_DESERIALIZER_CLASS_COMFIG, <span
        class="class">StringDeserializer</span>.class);
        <span class="var">configMap</span>.put(<span class="class">ConsumerConfig</span>.VALUE_DESERIALIZER_CLASS_COMFIG, <span
        class="class">StringDeserializer</span>.class);
        <span class="keyword">return</span> <span class="keyword">new</span> <span class="class">DefaultKafkaConsumerFactory</span>&lt;&gt;(<span
        class="var">configMap</span>);
    }
    <span class="comment">//ConcurrentKafkaListenerContainerFactory</span>
    <span class="annotation">@Bean</span>
    <span class="keyword">public</span> <span class="class">ConcurrentKafkaListenerContainerFactory</span>&lt;<span
        class="class">String</span>,<span class="class">String</span>&gt;
    <span class="method">concurrentKafkaListenerContainerFactory</span>() {

        <span class="class">ConcurrentKafkaListenerContainerFactory</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="var">containerFactory</span> = <span class="keyword">new</span>
        <span class="class">ConcurrentKafkaListenerContainerFactory</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt;();
        <span class="var">containerFactory</span>.setConsumerFactory(<span class="method">consumerFactory</span>());
        <span class="keyword">return</span> <span class="var">containerFactory</span>;
    }
}
</code>
</pre>
    <p><strong>ConsumerService Class: </strong> Uses @KafkaListener annotation to listen for messages:</p>

    <p><strong>@KafkaListener(topics = "payment-topic", groupId = "group-id"): </strong> Basic listener for a topic
        within a consumer
        group.</p>

    <pre class="java-code">
<code>
<span class="annotation">@Service</span>
<span class="annotation">@Slf4j</span>
<span class="keyword">public class</span> <span class="class">ConsumerService</span> {

    <span class="annotation">@Autowired</span>
    <span class="keyword">private</span> <span class="class">KafkaTemplate</span>&lt;<span
        class="class">String</span>,<span class="class">String</span>&gt; <span class="var">kafkaTemplate</span>;

    <span class="annotation">@KafkaListener</span>(topics = <span class="string">"payment-topic"</span>, groupId = <span
        class="string">"group_id"</span>)
    <span class="keyword">public</span> <span class="keyword">void</span> <span class="method">consume</span>(<span
        class="class">ConsumerRecord</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="var">message</span>) {
        log.info(<span class="string">"Key: {} | Value: {}"</span>, <span class="var">message</span>.key(), <span
        class="var">message</span>.value());
        log.info(<span class="string">"Partition: {} | Offset: {}"</span>, <span class="var">message</span>.partition(), <span
        class="var">message</span>.offset());
        <span class="class">Integer</span>.parseInt(<span class="var">message</span>.value());
    }
}
</code>
</pre>

    <p><strong>Accessing Message Details: </strong>The listener method can accept ConsumerRecord&lt;String, String&gt;
        to access key,
        value, partition, and offset.</p>

    <p><strong>Listening from Specific Partitions: </strong>@TopicPartition annotation within @KafkaListener allows
        specifying topic and
        a list of partitions to listen from (e.g., partitions = {"0", "1"}).</p>

    <pre class="java-code">
<code>
<span class="annotation">@KafkaListener</span>(
    <span class="var">topicPartitions</span> = <span class="annotation">@TopicPartition</span>(
        <span class="var">topic</span> = <span class="string">"payment-topic"</span>,
        <span class="var">partitions</span> = { <span class="string">"0"</span>, <span class="string">"1"</span> } <span
        class="comment">// consume only from partition 0 and 1</span>
    ),
    <span class="var">groupId</span> = <span class="string">"manual-partition-group"</span>
)
<span class="keyword">public</span> <span class="keyword">void</span> <span
        class="method">listenFromSpecificPartitions</span>(<span class="class">ConsumerRecord</span>&lt;<span
        class="class">String</span>, <span class="class">String</span>&gt; <span class="var">record</span>) {
    <span class="class">System</span>.out.printf(<span class="string">"Consumed from partition %d: %s%n"</span>, <span
        class="var">record</span>.partition(), <span class="var">record</span>.value());
}
</code>
</pre>

    <p><strong>Concurrency: </strong> The concurrency property in @KafkaListener allows starting multiple consumer
        instances (containers)
        for parallel message consumption (e.g., concurrency = "3").</p>

    <p>In <strong>Spring Kafka,</strong> concurrency defines the number of <strong>consumer threads</strong> (i.e., how
        many instances of your
        @KafkaListener will run in parallel) <strong>within the same application.</strong></p>

    <p>This only works when the topic has multiple partitions.</p>

    <p><strong>Topic:</strong> payment-topic</p>
    <p><strong>Partitions:</strong> 6</p>
    <p><strong>Concurrency:</strong> 3</p>

    <p>Spring Kafka will start 3 threads:</p>

    <ul>
        <li><strong>Thread 1</strong> → Partition 0, 1</li>
        <li><strong>Thread 2</strong> → Partition 2, 3</li>
        <li><strong>Thread 3</strong> → Partition 4, 5</li>
    </ul>

    <p>All listeners use the <strong>same group ID,</strong> hence each thread handles <strong>exclusive
        partitions.</strong></p>
    <hr>
    <h3>6.4 Producing and Consuming Custom Messages</h3>

    <p><strong>Custom Object Serialization/Deserialization: </strong>For custom object types (e.g., Location with id,
        locationName, and
        MessageMetadata), JsonSerializer (for producer value.serializer) and JsonDeserializer (for consumer
        value.deserializer) are used.</p>

    <pre class="java-code">
<code>
<span class="comment">//Producer</span>
<span class="annotation">@Bean</span>
<span class="keyword">public</span> <span class="class">ProducerFactory</span>&lt;<span
        class="class">String</span>, <span class="class">Location</span>&gt; <span class="method">LocationProducerFactory</span>()
{
    <span class="class">Map</span>&lt;<span class="class">String</span>, <span class="class">Object</span>&gt; <span
        class="var">configProps</span> = <span class="keyword">new</span> <span class="class">HashMap</span>&lt;&gt;();
    <span class="var">configProps</span>.put(<span class="class">ProducerConfig</span>.BODTSTRAP_SERVERS_CONFIG, <span
        class="string">"localhost:9092"</span>);
    <span class="var">configProps</span>.put(<span
        class="class">ProducerConfig</span>.KEY_SERIALIZER_CLASS_CONFIG, <span class="class">StringSerializer</span>.class);
    <span class="var">configProps</span>.put(<span
        class="class">ProducerConfig</span>.VALUE_SERIALIZER_CLASS_CONFIG, <span class="class">JsonSerializer</span>.class);
    <span class="keyword">return</span> <span class="keyword">new</span> <span
        class="class">DefaultKafkaProducerFactory</span>&lt;&gt;(<span class="var">configProps</span>);
}

<span class="annotation">@Bean</span>
<span class="keyword">public</span> <span class="class">KafkaTemplate</span>&lt;<span class="class">String</span>, <span
        class="class">Location</span>&gt; <span class="method">LocationKafkaTemplate</span>()
{
    <span class="keyword">return</span> <span class="keyword">new</span> <span class="class">KafkaTemplate</span>&lt;&gt;(<span
        class="method">LocationProducerFactory</span>());
}
</code>
</pre>
    <pre class="java-code">
<code>
<span class="comment">//Consumer</span>
<span class="annotation">@Bean</span>

<span class="keyword">public</span> <span class="class">ConsumerFactory</span>&lt;<span
        class="class">String</span>, <span class="class">Location</span>&gt; <span class="method">locationConsumerFactory</span>() {
    <span class="class">Map</span>&lt;<span class="class">String</span>, <span class="class">Object</span>&gt; <span
        class="var">props</span> = <span class="keyword">new</span> <span class="class">HashMap</span>&lt;&gt;();
    <span class="var">props</span>.put(<span class="class">ConsumerConfig</span>.BOUTSTRAP_SERVERS_COMFIG, <span
        class="string">"localhost:9092"</span>);
    <span class="var">props</span>.put(<span class="class">ConsumerConfig</span>.GROUP_ID_COMFIG, <span class="string">"location-group-id"</span>);
    <span class="var">props</span>.put(<span class="class">ConsumerConfig</span>.KFY_DESERTALIZER_CLASS_COMFIG, <span
        class="class">StringDeserializer</span>.class);
    <span class="var">props</span>.put(<span class="class">ConsumerConfig</span>.VALUE_DESERTALIZER_CLASS_COMFIG, <span
        class="class">JsonDeserializer</span>.class);
    <span class="keyword">return</span> <span class="keyword">new</span> <span
        class="class">DefaultKafkaConsumerFactory</span>&lt;&gt;(<span class="var">props</span>, <span class="keyword">new</span> <span
        class="class">StringBuilder</span>(), <span class="keyword">new</span> <span
        class="class">JsonDeserializer</span>(<span class="class">Location</span>.class));
}

<span class="annotation">@Bean</span>

<span class="keyword">public</span> <span class="class">ConcurrentKafkaListenerContainerFactory</span>&lt;<span
        class="class">String</span>, <span class="class">Location</span>&gt; <span
        class="method">locationListener</span>()
{
    <span class="class">ConcurrentKafkaListenerContainerFactory</span>&lt;<span class="class">String</span>, <span
        class="class">Location</span>&gt; <span class="var">factory</span> = <span class="keyword">new</span> <span
        class="class">ConcurrentKafkaListenerContainerFactory</span>&lt;&gt;();

    <span class="var">factory</span>.setConsumerFactory(<span class="method">locationConsumerFactory</span>());
    <span class="keyword">return</span> <span class="var">factory</span>;
}

<span class="annotation">@KafkaListener</span>(topics = <span class="string">"location-topic"</span>, groupId = <span
        class="string">"location-group-id"</span>, concurrency = <span class="string">"3"</span>)
<span class="keyword">public</span> <span class="keyword">void</span> <span class="method">listen</span>(<span
        class="class">ConsumerRecord</span>&lt;<span class="class">String</span>,<span class="class">Location</span>&gt; <span
        class="var">message</span>) {
    log.info(<span class="string">"Key: {} | Value: {}"</span>, <span class="var">message</span>.key(), <span
        class="var">message</span>.value());
    log.info(<span class="string">"Partition: {} | Offset: {}"</span>, <span
        class="var">message</span>.partition(), <span class="var">message</span>.offset());
}
</code>
</pre>

    <p>The MessageMetadata can include details like messagedd, sourceOriginator, and timestamp to track message origin
        and context.</p>

    <p><strong>Consumer Group ID:</strong> For custom message consumption, ensure a unique group.id is defined for each
        consumer group to correctly identify and coordinate consumers.</p>
    <hr>
    <h3>6.5 Consumer Partition Rebalance</h3>

    <p>When a new consumer joins a consumer group (with the same group.id), the GroupCoordinator triggers a <strong>rebalance</strong>.
    </p>

    <p><strong>Rebalancing</strong> involves changing partition ownership from one consumer to another to distribute the
        workload evenly among active consumers in the group. This ensures efficient parallel processing as consumers are
        assigned specific partitions.</p>
    <hr>
    <h3>6.6 Committing Offsets</h3>

    <ul>
        <li><strong>Offset Committing:</strong> When a consumer consumes messages, it needs to send an acknowledgement
            to the broker (or Kafka itself in newer versions) by committing its offset. This tracks the consumer's
            progress.
        </li>

        <li><strong>enable.auto.commit Property:</strong> If true, Kafka automatically commits offsets based on its
            internal configuration.
        </li>

        <li>If false (default since Spring for Apache Kafka 2.2.3), the container supports several acknowledgement
            modes.
        </li>

        <li><strong>Acknowledgement Modes (ackMode):BATCH (Default):</strong> "Commit the offset when all records
            written by the poll that have been processed." This means offsets are committed after a batch of messages
            has been successfully processed.
        </li>
    </ul>

    <ul>
        <li><strong>RECORD</strong>: "Commit the offset when The Listener returns after processing the Record here."
            Offsets are committed immediately after each individual record is processed.
        </li>

        <li>Other options include <strong>TIME, COUNT, MANUAL, MANUAL_IMMEDIATE</strong>. The choice depends on the
            application's reliability and performance requirements.
        </li>
    </ul>
    <hr>
    <h3>6.7 Error Handling in Kafka Consumers</h3>

    <p>Spring for Apache Kafka provides robust error handling mechanisms:</p>

    <ul>
        <li><strong>Blocking Retry</strong>: If a retryable exception occurs during message consumption, the consumer
            will retry processing the message.
        </li>

        <li>This approach <strong>blocks</strong> the consumption of subsequent messages until the current message is
            successfully processed or the retry attempts are exhausted.
        </li>

        <li>Default behavior: attempts to consume a message at least 10 times before moving to the next message and
            logging an error.
        </li>

        <li><strong>Non-Blocking Retry (@RetryableTopic):</strong> Uses @RetryableTopic annotation on the @KafkaListener
            method.
        </li>

        <li>When a message fails, it's sent to a dedicated <strong>retry topic</strong> (e.g.,
            payment-topic-retry-3000).
        </li>

        <li>Configurable with attempts (number of retries) and backoff (delay between retries, with optional
            multiplier).
        </li>

        <li>This prevents blocking the main consumer thread, allowing other messages to be processed while a failing
            message is retried on a separate topic.
        </li>

        <li><strong>Dead Letter Topic and Handler (@DttHandler):</strong> When messages exceed the maximum non-blocking
            retry attempts, they are automatically sent to a <strong>dead letter topic</strong> (DLT).
        </li>

        <li>A separate @DttHandler method is used to process these failed messages (e.g., for logging, alerting via
            email notifications, or further manual intervention).
        </li>

        <li>This ensures that messages that cannot be processed after multiple retries are not lost but are moved to a
            designated location for analysis and resolution.
        </li>
    </ul>

    <hr>

    <pre class="java-code">
<code>
<span class="annotation">@Service</span>
<span class="annotation">@Slf4j</span>
<span class="keyword">public class</span> <span class="class">ConsumerService</span> {

    <span class="annotation">@Autowired</span>
    <span class="keyword">private</span> <span class="class">KafkaTemplate</span>&lt;<span
        class="class">String</span>,<span class="class">String</span>&gt; <span class="var">kafkaTemplate</span>;

    <span class="annotation">@RetryableTopic</span>(attempts = <span class="string">"3"</span>, backoff = <span
        class="annotation">@Backoff</span>(delay = <span class="number">3000</span>, multiplier =<span
        class="number">2</span> ))
    <span class="annotation">@KafkaListener</span>(topics = <span class="string">"payment-topic"</span>, groupId = <span
        class="string">"group_id"</span>, containerFactory = <span class="string">"concurrentKafkaListenerContainerFactory"</span>)
    <span class="keyword">public</span> <span class="keyword">void</span> <span class="method">consume</span>(<span
        class="class">ConsumerRecord</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="var">message</span>) {
        log.info(<span class="string">"Key: {} | Value: {}"</span>, <span class="var">message</span>.key(), <span
        class="var">message</span>.value());
        log.info(<span class="string">"Partition: {} | Offset: {}"</span>, <span class="var">message</span>.partition(), <span
        class="var">message</span>.offset());
        <span class="class">Integer</span>.parseInt(<span class="var">message</span>.value());
    }
    <span class="annotation">@DltHandler</span>
    <span class="keyword">public</span> <span class="keyword">void</span> <span
        class="method">processFailureMessages</span>(<span class="class">ConsumerRecord</span>&lt;<span class="class">String</span>,<span
        class="class">String</span>&gt; <span class="var">message</span>){
        log.info(<span class="string">"Dead letter topic Key: {} | Value: {}"</span>, <span class="var">message</span>.key(), <span
        class="var">message</span>.value());
    }

<span class="comment">//Send email notifications about failure message</span>
</code>
</pre>

    <div class="table-container">
        <table class="basic-table">
            <thead>
            <tr>
                <th>Attribute</th>
                <th>Meaning</th>
            </tr>
            </thead>
            <tbody>
            <tr>
                <td>attempts = "3"</td>
                <td>Total of 3 delivery attempts (1 original + 2 retries).</td>
            </tr>
            <tr>
                <td>@Backoff(delay = 3000, multiplier = 2)</td>
                <td>Wait 3 seconds before first retry, then backoff doubles: 3s ~ 6s.</td>
            </tr>
            </tbody>
        </table>
    </div>

    <pre class="code-block">
<code>
<span class="number">1</span> 1st attempt (fails) -
<span class="number">2</span>  Wait 3 sec -
<span class="number">3</span>  2nd attempt (fails) -
<span class="number">4</span>  Wait 6 sec -
<span class="number">5</span>  3rd attempt (fails) -
<span class="number">6</span>  - Sent to Dead Letter Topic (DLT)
</code>
</pre>

    <p>Spring will <strong>auto-create </strong>retry topics and the DLT unless you override naming and handling.</p>

    <p>Retry the message 2 times (after 3s and 6s delays)</p>

    <p>Still fails? - <strong>Sent to payment-topic-dlt</strong></p>

    <p>The <strong>@DltHandler</strong> method handles it</p>

    <p><strong>@DltHandler</strong></p>

    <p>This method handles messages that failed <strong>even after retries.</strong></p>

    <ul>
        <li>Messages from the <strong>Dead Letter Topic</strong> will be sent to this method.</li>
        <li>You can log them, alert teams, store in DB, etc.</li>
        <li>Spring Kafka (if not configured otherwise) will auto-create:
            <ul>
                <li>Retry topics like:
                    <strong>payment-topic-retry-0, payment-topic-retry-1, etc.</strong>
                </li>
                <li>Dead-letter topic:
                    <strong>payment-topic-dlt</strong>
                </li>
            </ul>
        </li>
    </ul>

    <p>You can override topic names if needed.</p>
    <br>
    <br>
</main>

<!-- Scripts -->
<script src="js/sidebar.js"></script>
<script src="js/componentLoader.js"></script>
</body>
</html>